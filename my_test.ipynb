{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a689d377-9c8a-4006-b249-5f3936b1dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./env/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.9/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in ./env/lib/python3.9/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./env/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./env/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.9/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./env/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./env/lib/python3.9/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in ./env/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./env/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in ./env/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in ./env/lib/python3.9/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./env/lib/python3.9/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./env/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./env/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !python3.9 -m pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82871336-64a6-4f6c-ad1d-647f8e841505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24232a71-be07-422c-a3ee-1230866cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sql(data_dir: str = \"data_sql\"):    \n",
    "    dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
    "\n",
    "    dataset_splits = {\"train\": dataset[\"train\"]}\n",
    "    name = \"dataset.json\"\n",
    "    out_path = Path(\"/media/dsl/1A2226C62D41B5A2/dataset/dataset.json\")\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for key, ds in dataset_splits.items():\n",
    "        with open(out_path, \"w\") as f:\n",
    "            for item in ds:\n",
    "                newitem = {\n",
    "                    \"input\": item[\"question\"],\n",
    "                    \"context\": item[\"context\"],\n",
    "                    \"output\": item[\"answer\"],\n",
    "                }\n",
    "                f.write(json.dumps(newitem) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f6e9de-2d72-479f-8851-3582d4f674aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b682aebe-51e5-4d4e-921e-fe9f24a8f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsl/Documents/fbk/modal_finetune_sql/src/common.py:57: DeprecationError: 2024-03-01: Deprecated! Use `NetworkFileSystem.from_name(name, create_if_missing=True)`.\n",
      "  output_vol = NetworkFileSystem.new(cloud=\"gcp\").persisted(\"doppelbot-vol\")\n"
     ]
    }
   ],
   "source": [
    "# from src.finetune_sql import _train\n",
    "from src.common import (\n",
    "    VOL_MOUNT_PATH,\n",
    "    WANDB_PROJECT,\n",
    "    output_vol,\n",
    "    stub,\n",
    "    get_data_path,\n",
    "    get_model_path,\n",
    "    generate_prompt_sql,\n",
    ")\n",
    "from datetime import datetime\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49ec991-2127-46b9-93b9-16fb60bdfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adapter from https://github.com/tloen/alpaca-lora/blob/65fb8225c09af81feb5edb1abb12560f02930703/finetune.py\n",
    "# with modifications mainly to expose more parameters to the user.\n",
    "def _train(\n",
    "    # model/data params\n",
    "    base_model: str,\n",
    "    data,\n",
    "    output_dir: str = \"./lora-alpaca\",\n",
    "    eval_steps: int = 20,\n",
    "    save_steps: int = 20,\n",
    "    # training hyperparams\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 32,\n",
    "    max_steps: int = 200,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 512,\n",
    "    val_set_size: int = 100,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 16,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = True,\n",
    "    group_by_length: bool = True,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "):\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    import torch\n",
    "    import transformers\n",
    "    from peft import (\n",
    "        LoraConfig,\n",
    "        get_peft_model,\n",
    "        get_peft_model_state_dict,\n",
    "        prepare_model_for_int8_training,\n",
    "        set_peft_model_state_dict,\n",
    "    )\n",
    "    from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0)\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    # model = LlamaForCausalLM.from_pretrained(\n",
    "    #     model_path, torch_dtype=torch.float16, device_map='auto',\n",
    "    # )\n",
    "    \n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        torch_dtype=torch.float16, device_map='auto', offload_folder=\"offload\",\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model, add_eos_token=True)\n",
    "\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = generate_prompt_sql(\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"context\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            raise NotImplementedError(\"not implemented yet\")\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(resume_from_checkpoint, \"pytorch_model.bin\")  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = False  # So the trainer won't try loading its state\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\n",
    "        train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=eval_steps if val_set_size > 0 else None,\n",
    "            save_steps=save_steps,\n",
    "            output_dir=output_dir,\n",
    "            # save_total_limit=3,\n",
    "            load_best_model_at_end=False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else \"none\",\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "        model, type(model)\n",
    "    )\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\"\\n If there's a warning about missing keys above, please disregard :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7de83e1-b000-4c99-a83b-ac0f958ef101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(data_dir: str = \"data_sql\", model_dir: str = \"data_sql\"):\n",
    "    data_path = Path(\"/media/dsl/1A2226C62D41B5A2/dataset/dataset.json\")\n",
    "    output_path = Path(\"/media/dsl/1A2226C62D41B5A2/dataset/output\")\n",
    "    data = load_dataset(\"json\", data_files=data_path.as_posix())\n",
    "    MODEL_PATH = \"openlm-research/open_llama_7b_v2\"\n",
    "    num_samples = len(data[\"train\"])\n",
    "    val_set_size = ceil(0.1 * num_samples)\n",
    "    print(f\"Loaded {num_samples} samples. \")\n",
    "\n",
    "    _train(\n",
    "        MODEL_PATH,\n",
    "        data,\n",
    "        val_set_size=val_set_size,\n",
    "        output_dir=output_path,\n",
    "        wandb_project=WANDB_PROJECT,\n",
    "        wandb_run_name=f\"openllama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e11e33-2f62-4ed6-82cb-256cfe81dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3.9 -m pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723c6ba7-d731-445d-b9d4-d07267e0ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3.9 -m pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7244cb1-1faf-4860-9e16-4ce44ead53e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a602d2c10f48bebb967d646ae7877c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78577 samples. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5941292b1de44cd19e52916a768b8728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782ad35ad3924897971d8e3f685d1940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2010db390145a78cfb228ef44e5e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce44fbee4e44457a03309e658cf26b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542a1f384fe142c1a586781c1d093691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672edb62cfba443487968418917c9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba2079-c3fc-4d84-927d-06f7b3e6a259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
